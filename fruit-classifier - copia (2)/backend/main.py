from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import numpy as np
from PIL import Image
import io
import tensorflow as tf
import time
from datetime import datetime
import os

# Crear la aplicaci√≥n FastAPI
app = FastAPI(title="Clasificador de Frutas con IA", version="1.0.0")

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En producci√≥n usar dominios espec√≠ficos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuraci√≥n del modelo
MODEL_PATH = "model/best_model.keras"
IMAGE_SIZE = (150, 150)  # ‚úÖ CORREGIDO: Tama√±o correcto del modelo
INPUT_SHAPE = (150, 150, 3)  # ‚úÖ Shape esperado por el modelo

# Lista de clases de frutas (ajusta seg√∫n tu modelo)
FRUIT_CLASSES = [
    "Pera", "Ciruela", "Manzana", "Naranja"
]

# Variables globales
model = None
model_loaded = False

def load_model():
    """Carga el modelo TensorFlow"""
    global model, model_loaded
    
    try:
        print("ü§ñ === CARGANDO MODELO DE TENSORFLOW ===")
        print(f"üìÅ Ruta del modelo: {MODEL_PATH}")
        print(f"üìê Tama√±o de entrada esperado: {INPUT_SHAPE}")
        
        if not os.path.exists(MODEL_PATH):
            print(f"‚ùå ERROR: No se encontr√≥ el modelo en {MODEL_PATH}")
            model_loaded = False
            return False
        
        # Cargar el modelo
        model = tf.keras.models.load_model(MODEL_PATH)
        
        # Verificar la forma de entrada del modelo
        input_shape = model.input_shape
        print(f"‚úÖ Modelo cargado exitosamente")
        print(f"üìä Shape de entrada del modelo: {input_shape}")
        print(f"üéØ Clases disponibles: {len(FRUIT_CLASSES)}")
        
        # Verificar compatibilidad
        expected_shape = (None, 150, 150, 3)
        if input_shape == expected_shape:
            print("‚úÖ Compatibilidad de dimensiones VERIFICADA")
            model_loaded = True
            return True
        else:
            print(f"‚ö†Ô∏è  ADVERTENCIA: Shape esperado {expected_shape}, encontrado {input_shape}")
            model_loaded = True  # Continuar de todas formas
            return True
            
    except Exception as e:
        print(f"‚ùå ERROR cargando modelo: {str(e)}")
        model_loaded = False
        return False

def preprocess_image(image_bytes: bytes) -> np.ndarray:
    """
    ‚úÖ FUNCI√ìN CORREGIDA: Preprocesa imagen para el modelo (150x150)
    """
    try:
        print(f"üîÑ === PREPROCESANDO IMAGEN ===")
        
        # 1. Abrir la imagen
        image = Image.open(io.BytesIO(image_bytes))
        print(f"üì∑ Imagen original: {image.size} p√≠xeles, modo: {image.mode}")
        
        # 2. Convertir a RGB si es necesario
        if image.mode != 'RGB':
            image = image.convert('RGB')
            print(f"üîÑ Convertida a RGB")
        
        # 3. ‚úÖ CORRECCI√ìN CR√çTICA: Redimensionar a 150x150 (NO 224x224)
        image_resized = image.resize(IMAGE_SIZE, Image.Resampling.LANCZOS)
        print(f"üìê Redimensionada a: {image_resized.size} (CORRECTO: 150x150)")
        
        # 4. Convertir a array numpy
        img_array = np.array(image_resized, dtype=np.float32)
        print(f"üî¢ Array shape: {img_array.shape}")
        
        # 5. Normalizar p√≠xeles (0-255 ‚Üí 0-1)
        img_array = img_array / 255.0
        print(f"üìä Normalizado: min={img_array.min():.3f}, max={img_array.max():.3f}")
        
        # 6. A√±adir dimensi√≥n de batch
        img_array = np.expand_dims(img_array, axis=0)
        print(f"üéØ Shape final para modelo: {img_array.shape} (esperado: (1, 150, 150, 3))")
        
        # 7. Verificaci√≥n final
        expected_shape = (1, 150, 150, 3)
        if img_array.shape == expected_shape:
            print("‚úÖ Shape CORRECTO para el modelo")
            return img_array
        else:
            raise ValueError(f"Shape incorrecto: esperado {expected_shape}, obtenido {img_array.shape}")
        
    except Exception as e:
        print(f"‚ùå Error en preprocesamiento: {str(e)}")
        raise e

def make_prediction(img_array: np.ndarray) -> tuple:
    """
    Realiza la predicci√≥n usando el modelo cargado
    """
    try:
        print("ü§ñ === REALIZANDO PREDICCI√ìN ===")
        
        if not model_loaded or model is None:
            raise ValueError("Modelo no cargado correctamente")
        
        # Realizar predicci√≥n
        start_time = time.time()
        predictions = model.predict(img_array, verbose=0)
        prediction_time = (time.time() - start_time) * 1000  # en ms
        
        print(f"‚è±Ô∏è  Tiempo de predicci√≥n: {prediction_time:.2f}ms")
        print(f"üìä Shape de predicciones: {predictions.shape}")
        
        # Obtener la clase con mayor probabilidad
        predicted_class_idx = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class_idx])
        
        # Verificar √≠ndice v√°lido
        if predicted_class_idx >= len(FRUIT_CLASSES):
            raise ValueError(f"√çndice de clase inv√°lido: {predicted_class_idx}")
        
        predicted_fruit = FRUIT_CLASSES[predicted_class_idx]
        confidence_percentage = confidence * 100
        
        print(f"üéØ Predicci√≥n: {predicted_fruit}")
        print(f"üìà Confianza: {confidence_percentage:.2f}%")
        print(f"üî¢ √çndice de clase: {predicted_class_idx}")
        
        return predicted_fruit, confidence_percentage, prediction_time
        
    except Exception as e:
        print(f"‚ùå Error en predicci√≥n: {str(e)}")
        raise e

# Inicializar la aplicaci√≥n
@app.on_event("startup")
async def startup_event():
    """Evento de inicio: cargar el modelo"""
    print("üöÄ === INICIANDO CLASIFICADOR DE FRUTAS ===")
    success = load_model()
    if success:
        print("‚úÖ Sistema listo para clasificar frutas")
    else:
        print("‚ùå Sistema iniciado con errores en el modelo")
    print("=" * 60)

@app.get("/")
async def root():
    return {
        "message": "üçé Clasificador de Frutas con IA",
        "status": "running",
        "model_loaded": model_loaded,
        "input_size": list(IMAGE_SIZE),
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model_loaded,
        "model_path": MODEL_PATH,
        "image_size": list(IMAGE_SIZE),
        "input_shape": list(INPUT_SHAPE),
        "available_fruits": len(FRUIT_CLASSES),
        "fruits": FRUIT_CLASSES,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict")
async def predict_fruit(file: UploadFile = File(...)):
    start_time = time.time()
    
    try:
        print(f"\nüì§ === NUEVA PREDICCI√ìN ===")
        print(f"üìÅ Archivo: {file.filename}")
        print(f"üìã Tipo MIME: {file.content_type}")
        print(f"‚è∞ Timestamp: {datetime.now().strftime('%H:%M:%S')}")
        
        # Verificar que el modelo est√© cargado
        if not model_loaded or model is None:
            print("‚ùå Modelo no cargado")
            raise HTTPException(
                status_code=503,
                detail="Modelo no disponible. Reinicia el servidor."
            )
        
        # Validar tipo de archivo
        if not file.content_type or not file.content_type.startswith("image/"):
            print(f"‚ùå Tipo de archivo inv√°lido: {file.content_type}")
            raise HTTPException(
                status_code=400,
                detail=f"Tipo de archivo no v√°lido: {file.content_type}. Se requiere una imagen."
            )
        
        # Leer los bytes del archivo
        image_bytes = await file.read()
        file_size_mb = len(image_bytes) / (1024 * 1024)
        print(f"üì¶ Tama√±o del archivo: {file_size_mb:.2f} MB ({len(image_bytes)} bytes)")
        
        # Validar tama√±o
        if len(image_bytes) == 0:
            print("‚ùå Archivo vac√≠o")
            raise HTTPException(status_code=400, detail="El archivo est√° vac√≠o")
        
        if len(image_bytes) > 10 * 1024 * 1024:  # 10MB m√°ximo
            print(f"‚ùå Archivo demasiado grande: {file_size_mb:.2f} MB")
            raise HTTPException(status_code=400, detail="El archivo es demasiado grande (m√°ximo 10MB)")
        
        # ‚úÖ PREPROCESAMIENTO CORREGIDO
        print("üîÑ Preprocesando imagen...")
        img_array = preprocess_image(image_bytes)
        
        # ‚úÖ PREDICCI√ìN CON MODELO REAL
        print("ü§ñ Realizando predicci√≥n con modelo...")
        predicted_fruit, confidence, prediction_time = make_prediction(img_array)
        
        # Calcular tiempo total
        total_time = round((time.time() - start_time) * 1000, 2)  # en ms
        
        print(f"‚úÖ === PREDICCI√ìN COMPLETADA ===")
        print(f"   üçé Fruta predicha: {predicted_fruit}")
        print(f"   üìä Confianza: {confidence:.2f}%")
        print(f"   ‚è±Ô∏è  Tiempo total: {total_time}ms")
        print(f"   üéØ Modelo: {MODEL_PATH}")
        
        # ‚úÖ RESPUESTA EN FORMATO ESPERADO POR EL FRONTEND
        response = {
            "fruit": predicted_fruit,
            "confidence": round(confidence, 2),
            "status": "success",
            "model_loaded": True,
            "filename": file.filename,
            "image_size": list(IMAGE_SIZE),
            "processing_time": f"{total_time}ms",
            "prediction_time": f"{prediction_time:.2f}ms",
            "file_size_mb": round(file_size_mb, 2),
            "input_shape": list(INPUT_SHAPE),
            "timestamp": datetime.now().isoformat()
        }
        
        print("üì§ Enviando respuesta al frontend...")
        return JSONResponse(content=response)
        
    except HTTPException as he:
        print(f"‚ùå HTTP Exception: {he.detail}")
        raise he
    except Exception as e:
        error_msg = f"Error en la predicci√≥n: {str(e)}"
        print(f"‚ùå Error inesperado: {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/test")
async def test_endpoint():
    """Endpoint para probar que el servidor est√° funcionando"""
    return {
        "message": "‚úÖ Clasificador de Frutas funcionando correctamente",
        "model_status": "loaded" if model_loaded else "not_loaded",
        "fruits": FRUIT_CLASSES,
        "image_size": list(IMAGE_SIZE),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/model-info")
async def model_info():
    """Informaci√≥n detallada del modelo"""
    if not model_loaded or model is None:
        return {"error": "Modelo no cargado"}
    
    try:
        return {
            "model_loaded": model_loaded,
            "input_shape": model.input_shape,
            "output_shape": model.output_shape,
            "expected_size": list(IMAGE_SIZE),
            "classes": FRUIT_CLASSES,
            "num_classes": len(FRUIT_CLASSES),
            "model_path": MODEL_PATH
        }
    except Exception as e:
        return {"error": f"Error obteniendo info del modelo: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    print("\nüöÄ === INICIANDO SERVIDOR DE CLASIFICACI√ìN ===")
    print("üåê URL: http://127.0.0.1:8000")
    print("üìñ Docs: http://127.0.0.1:8000/docs")
    print("üß™ Test: http://127.0.0.1:8000/test")
    print("ü§ñ Model Info: http://127.0.0.1:8000/model-info")
    print(f"üìê Tama√±o de imagen: {IMAGE_SIZE}")
    print(f"üçé Frutas soportadas: {len(FRUIT_CLASSES)}")
    print("=" * 60)
    
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )